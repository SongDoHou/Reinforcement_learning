{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree node class definition\n",
    "class TreeNode():\n",
    "    def __init__(self, env, parent, done):\n",
    "        # init associated board state\n",
    "        self.env = env\n",
    "        \n",
    "        # init is node terminal flag\n",
    "        self.is_terminal = done\n",
    "        self.is_fully_expanded = self.is_terminal\n",
    "        self.parent = parent\n",
    "        self.visits = 0\n",
    "        self.score = 0\n",
    "        self.children = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCTS class definition\n",
    "class MCTS_FOMDP():\n",
    "    # search for the best move in the current position\n",
    "    def search(self, initial_state, iteration=1000):\n",
    "        # create root node\n",
    "        self.root = TreeNode(initial_state, None, False)\n",
    "\n",
    "        # walk through 1000 iterations\n",
    "        for iteration in range(iteration):\n",
    "            # select a node (selection phase)\n",
    "            node = self.select(self.root)\n",
    "            \n",
    "            # scrore current node (simulation phase)\n",
    "            score = self.simulation(node.env)\n",
    "            \n",
    "            # backpropagate results\n",
    "            self.backpropagate(node, score)\n",
    "        \n",
    "        # pick up the best move in the current position\n",
    "        try:\n",
    "            return self.get_best_move(self.root, 0)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # select most promising node\n",
    "    def select(self, node):\n",
    "        # make sure that we're dealing with non-terminal nodes\n",
    "        while not node.is_terminal:\n",
    "            # case where the node is fully expanded\n",
    "            if node.is_fully_expanded:\n",
    "                node = self.get_best_move(node, 2)\n",
    "            \n",
    "            # case where the node is not fully expanded \n",
    "            else:\n",
    "                # otherwise expand the node\n",
    "                return self.expand(node)\n",
    "       # return node\n",
    "        return node\n",
    "\n",
    "    # expand node\n",
    "    def expand(self, node):\n",
    "        # gym environment의 action_space는 integer\n",
    "        actions = node.env.action_space.n\n",
    "        parent_config = node.env.unwrapped.spec.id\n",
    "        parent_state = node.env.unwrapped.state\n",
    "        parent_env = gym.make(parent_config)\n",
    "        parent_env.reset()\n",
    "        parent_env.unwrapped.state = parent_state\n",
    "\n",
    "        # loop over generated states (moves)\n",
    "        for action in range(actions):\n",
    "            # make sure that current state (move) is not present in child nodes\n",
    "            if action not in node.children:\n",
    "                # create a new node\n",
    "                _,_,done,_ = parent_env.step(action)\n",
    "                new_node = TreeNode(parent_env, node, done)\n",
    "                \n",
    "                # add child node to parent's node children list (dict)\n",
    "                node.children[action] = new_node\n",
    "                \n",
    "                # Restore parent state\n",
    "                parent_env.unwrapped.state = parent_state\n",
    "\n",
    "                # case when node is fully expanded\n",
    "                if len(range(actions)) == len(node.children):\n",
    "                    node.is_fully_expanded = True\n",
    "                \n",
    "                # return newly created node\n",
    "                return new_node\n",
    "        \n",
    "        # debugging\n",
    "        print('접근 안되는 구간입니다!!!') # 디버그 용도\n",
    "\n",
    "    # simulate the game via making random moves until reach end of the game\n",
    "    def simulation(self, env):\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return total_reward\n",
    "        \n",
    "\n",
    "    # backpropagate the number of visits and score up to the root node\n",
    "    def backpropagate(self, node, score, gamma=0.99):\n",
    "        # update nodes's up to root node\n",
    "        while node is not None:\n",
    "            # update node's score\n",
    "            node.score = gamma*node.score*node.visits + score\n",
    "            node.score /= (node.visits+1)\n",
    "            \n",
    "            # update node's visits\n",
    "            node.visits += 1\n",
    "            node = node.parent\n",
    "            \n",
    "    # select the best node basing on UCB1 formula\n",
    "    def get_best_move(self, node, C_const=2):\n",
    "        best_score = -np.inf\n",
    "        best_moves = []\n",
    "\n",
    "        for child_node in node.children.values():        \n",
    "            # get move score using UCB formula\n",
    "            #move_score = current_player*child_node.score/child_node.visits+\\\n",
    "            move_score = child_node.score+\\\n",
    "                C_const*np.sqrt(np.log(node.visits/child_node.visits))\n",
    "\n",
    "            # better move has been found\n",
    "            if move_score > best_score:\n",
    "                best_score = move_score\n",
    "                best_moves = [child_node]\n",
    "\n",
    "            # found as good move as alread avilable\n",
    "            elif move_score == best_score:\n",
    "                best_moves.append(child_node)\n",
    "\n",
    "        # return one of the best moves randomly\n",
    "        return np.random.choice(best_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyunseok.hwang/anaconda3/envs/RL_scratch/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:163: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "<__main__.TreeNode object at 0x7f6d9bb6d580> (<class '__main__.TreeNode'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [55], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     action \u001b[39m=\u001b[39m mcts\u001b[39m.\u001b[39msearch(env)\n\u001b[0;32m---> 10\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     11\u001b[0m     total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward \n\u001b[1;32m     12\u001b[0m     action_list\u001b[39m.\u001b[39mappend(action)\n",
      "File \u001b[0;32m~/anaconda3/envs/RL_scratch/lib/python3.8/site-packages/gym/wrappers/time_limit.py:17\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m---> 17\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/anaconda3/envs/RL_scratch/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:13\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     12\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[0;32m~/anaconda3/envs/RL_scratch/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:118\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    117\u001b[0m     err_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m!r}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(action)\u001b[39m}\u001b[39;00m\u001b[39m) invalid\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 118\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mcontains(action), err_msg\n\u001b[1;32m    119\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCall reset before using step method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m     x, x_dot, theta, theta_dot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n",
      "\u001b[0;31mAssertionError\u001b[0m: <__main__.TreeNode object at 0x7f6d9bb6d580> (<class '__main__.TreeNode'>) invalid"
     ]
    }
   ],
   "source": [
    "mcts = MCTS_FOMDP()\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "done = False\n",
    "action_list = []\n",
    "total_reward = 0\n",
    "while True:\n",
    "    action = mcts.search(env)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward \n",
    "    action_list.append(action)\n",
    "    if done:\n",
    "        break\n",
    "print(f'Total reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01197783, -0.02286854, -0.03317878, -0.01879819], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0306542 ,  0.00124451, -0.04181773,  0.03405016])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gravity': 9.8,\n",
       " 'masscart': 1.0,\n",
       " 'masspole': 0.1,\n",
       " 'total_mass': 1.1,\n",
       " 'length': 0.5,\n",
       " 'polemass_length': 0.05,\n",
       " 'force_mag': 10.0,\n",
       " 'tau': 0.02,\n",
       " 'kinematics_integrator': 'euler',\n",
       " 'theta_threshold_radians': 0.20943951023931953,\n",
       " 'x_threshold': 2.4,\n",
       " 'action_space': Discrete(2),\n",
       " 'observation_space': Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32),\n",
       " 'screen': None,\n",
       " 'clock': None,\n",
       " 'isopen': True,\n",
       " 'state': array([-0.0306542 ,  0.00124451, -0.04181773,  0.03405016]),\n",
       " 'steps_beyond_done': None,\n",
       " 'spec': EnvSpec(entry_point='gym.envs.classic_control:CartPoleEnv', reward_threshold=195.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, kwargs={}, namespace=None, name='CartPole', version=0),\n",
       " '_np_random': RandomNumberGenerator(PCG64) at 0x7F6D9BC16220}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eda1c160e83cbc6e162d86f3ac820d0e78df5de1466a2ca6fc33fee2ec17e6f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
