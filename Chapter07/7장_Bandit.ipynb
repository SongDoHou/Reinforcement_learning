{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ff10c2",
   "metadata": {},
   "source": [
    "## Chapter 07. MonteCarlo-TreeSearch Bandits\n",
    "---\n",
    "- 목표: MCTS를 이용해 gym text문제 FrozenLake, gym-chess를 해결한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b704a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandits 알고리즘부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d76e1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661bec17",
   "metadata": {},
   "source": [
    "## Bernoulli Bandit\n",
    "---\n",
    "베르누이 밴딧 모형\n",
    "\n",
    "설명\n",
    "\n",
    "- 에이전트가 행동 $A$선택에서 $K$개의 옵션이 있다고 가정(K개의 팔이 달린 슬롯머신 상상)\n",
    "\n",
    "- 특정 행동이 보상 $r$이 1이나오는지 알 수 없음\n",
    "\n",
    "- $K$번째 행동선택할 확률은다음과 같다. $0 \\le P_{K}\\le1$\n",
    "\n",
    "- 횟수를 반복함에 따라 실패에 대한 Regret를 최소화 하기위해 아래 식을 이용\n",
    "\n",
    "$$ \\rho = T\\theta^* - \\sum_{t=1}^{T}r_{t}$$\n",
    "\n",
    "여기서 $\\theta^*=max_{k}\\theta_k$\n",
    "\n",
    "---\n",
    "\n",
    "현실세계 대응 - [의료현장](https://arxiv.org/pdf/1507.08025.pdf)\n",
    "\n",
    "1. $K$종류의 약과 $T$명의 환자들이 있음.\n",
    "\n",
    "2. $P_{K}$ 확률로 치료한다면 어떤 약을 투여하는게 가장 좋을까요?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98add6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 밴딧 모형 만들기\n",
    "class BernoulliBandit():\n",
    "    def __init__(self,n_actions=10):\n",
    "        '''\n",
    "        입력\n",
    "            n_actions: 취할수 있는 행동의 개수, 기 서술한 문서의 K와 대응\n",
    "        필요 속성\n",
    "            Ps: 각각 행동의 선택확률\n",
    "            pull: 액션을 취함\n",
    "            max_reward: 최고의 행동 (theta*)\n",
    "        '''\n",
    "        self.Ps = np.random.random(n_actions)\n",
    "    def pull(self,action):\n",
    "        '''\n",
    "        행동을 개시함\n",
    "        입력:\n",
    "            action: 행동의 인덱스, integer\n",
    "        출력:\n",
    "            P[action]이 수행되었는지여부, 0: 미실행, 1: 실행\n",
    "        '''\n",
    "        return 0 if np.any(np.random.random() > self.Ps[action]) else 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f576fab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Agents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 에이전트 제작\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAgents\u001b[39;00m():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m,n_actions):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m        행동 초기화\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m        입력\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m            행동을 취한 횟수 -> 0\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m        '''\u001b[39;00m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mAgents\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_failures[action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 랜덤 에이전트 -> 기준없이 마구 잡이로 행동하는 에이전트\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRandomAgent\u001b[39;00m(\u001b[43mAgents\u001b[49m):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    모든 정보들은 Agents로 부터 상속\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    -> get_action만 작성\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Agents' is not defined"
     ]
    }
   ],
   "source": [
    "# 에이전트 제작\n",
    "class Agents():\n",
    "    def init_actions(self,n_actions):\n",
    "        '''\n",
    "        행동 초기화\n",
    "        입력\n",
    "            n_action: 에이전트가 취할수 있는 행동수\n",
    "        수행\n",
    "            각 행동의 성공, 실패 횟수 -> 0\n",
    "            행동을 취한 횟수 -> 0\n",
    "        '''\n",
    "        self.n_actions = n_actions\n",
    "        self._successes = np.zeros(n_actions)\n",
    "        self._failures = np.zeros(n_actions)\n",
    "        self._taken_actions = 0\n",
    "    \n",
    "    def get_action(self):\n",
    "        '''\n",
    "        어떤 행동을 취할것인가 -> 샘플링 방법에서 기술\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def update(self,action,reward):\n",
    "        '''\n",
    "        기록 -> 성공, 실패여부 및 행동을 취한 횟수\n",
    "        입력:\n",
    "            action: 몇번째 행동, integer\n",
    "            reward: 성공,실패여부, float\n",
    "        '''\n",
    "        self._taken_actions+=1\n",
    "        if reward == 1:\n",
    "            self._successes[action] += 1\n",
    "        else:\n",
    "            self._failures[action] += 1\n",
    "        \n",
    "    # 랜덤 에이전트 -> 기준없이 마구 잡이로 행동하는 에이전트\n",
    "    class RandomAgent(Agents):\n",
    "        '''\n",
    "        모든 정보들은 Agents로 부터 상속\n",
    "        -> get_action만 작성\n",
    "        '''\n",
    "        def get_action(self):\n",
    "            '''\n",
    "            임의의 행동 추출\n",
    "            '''\n",
    "            return np.random.choice(np.arange(self.n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3922e6a0",
   "metadata": {},
   "source": [
    "## Agent1. Epsilon-greedy agent\n",
    "---\n",
    "$do\\ while\\ until\\ convergence$\n",
    "    \n",
    "&nbsp;$for\\ k=1...,K \\ do$\n",
    "\n",
    "&nbsp;&nbsp; $\\theta_{k} \\leftarrow {\\alpha_{k}}/{(\\alpha_k + \\beta_k)} $\n",
    "\n",
    "&nbsp; $end \\ for$\n",
    "\n",
    "&nbsp; 액션 $x$ 선택 $with \\ \\epsilon - greedy \\ algorithm$ \n",
    "\n",
    "&nbsp; 액션 $x$ 수행 및 $r_x$ 획득\n",
    "\n",
    "&nbsp; $(\\alpha_x , \\beta_x) \\leftarrow (\\alpha_x , \\beta_x)+(r_x,1-r_x)$\n",
    "\n",
    "$end \\ do$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a185377",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Agents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEpsilonGreedyAgent\u001b[39;00m(\u001b[43mAgents\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m=\u001b[39m epsilon\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Agents' is not defined"
     ]
    }
   ],
   "source": [
    "class EpsilonGreedyAgent(Agents):\n",
    "    def __init__(self,epsilon=0.01):\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def get_action(self):\n",
    "        if np.random.random() < self.speilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            return np.argmax(self._succcesses/(self._successes+self._failures))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4204b3e3",
   "metadata": {},
   "source": [
    "## Agent2. Upper Confidence Bound agent (UCB)\n",
    "---\n",
    "$\\epsilon - greedy$는 행동선택에 특정 선호도가 부여되어있지 않음. Exploitation, Exploration의 적절 균형\n",
    "\n",
    "$do\\ while\\ until\\ convergence$\n",
    "    \n",
    "&nbsp;$for\\ k=1...,K \\ do$\n",
    "\n",
    "&nbsp;&nbsp; $\\theta_{k} \\leftarrow {\\alpha_{k}}/{(\\alpha_k + \\beta_k)}+\\sqrt{2logN}/(\\alpha_k + \\beta_k) $\n",
    "\n",
    "&nbsp;&nbsp; $where, \\ N$ 현 상태 방문횟수\n",
    "\n",
    "&nbsp; $end \\ for$\n",
    "\n",
    "&nbsp; 액션 $x$ 선택 $with\\ argmax(\\theta_k)$ \n",
    "\n",
    "&nbsp; 액션 $x$ 수행 및 $r_x$ 획득\n",
    "\n",
    "&nbsp; $(\\alpha_x , \\beta_x) \\leftarrow (\\alpha_x , \\beta_x)+(r_x,1-r_x)$\n",
    "\n",
    "$end \\ do$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e92aa4d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Agents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mUCBAgent\u001b[39;00m(\u001b[43mAgents\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      3\u001b[0m         Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_successes\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_successes \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_failures)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Agents' is not defined"
     ]
    }
   ],
   "source": [
    "class UCBAgent(Agents):\n",
    "    def get_action(self):\n",
    "        Q = self._successes/(self._successes + self._failures)\n",
    "        UCB = np.sqrt(2*np.log10(self._taken_actions)/(self._successes+self._failures))\n",
    "        return np.argmax(Q+UCB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c2aa0",
   "metadata": {},
   "source": [
    "## Agent3. Thompson sampling agent\n",
    "---\n",
    "보상의 분포를 안다면? - [Thompson sampling](https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf)\n",
    "\n",
    "$do\\ while\\ until\\ convergence$\n",
    "    \n",
    "&nbsp;$for\\ k=1...,K \\ do$\n",
    "\n",
    "&nbsp;&nbsp; 샘플링 $\\theta_k \\sim Beta(\\alpha_k,\\beta_k)$\n",
    "\n",
    "&nbsp;&nbsp; $where, \\ N$ 현 상태 방문횟수\n",
    "\n",
    "&nbsp; $end \\ for$\n",
    "\n",
    "&nbsp; 액션 $x$ 선택 $with\\ argmax(\\theta_k)$ \n",
    "\n",
    "&nbsp; 액션 $x$ 수행 및 $r_x$ 획득\n",
    "\n",
    "&nbsp; $(\\alpha_x , \\beta_x) \\leftarrow (\\alpha_x , \\beta_x)+(r_x,1-r_x)$\n",
    "\n",
    "$end \\ do$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4964b355",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Agents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mThopsonSamplingAgent\u001b[39;00m(\u001b[43mAgents\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mbeta(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuccesses\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_failures\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Agents' is not defined"
     ]
    }
   ],
   "source": [
    "class ThopsonSamplingAgent(Agents):\n",
    "    def get_action(self):\n",
    "        return np.argmax(np.random.beta(self.successes+1,self._failures+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8c865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_scratch",
   "language": "python",
   "name": "rl_scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
