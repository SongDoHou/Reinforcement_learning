{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym solving - TRPO or PPO\n",
    "---\n",
    "- TRPO(Trust Region Policy Optimization)\n",
    "\n",
    "- PPO(Proximal Policy Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 셀은 필자의 Jupyter notebook 환경 문제로 인해 작성되었습니다\n",
    "import os\n",
    "try:\n",
    "    os.environ[\"DISPLAY\"]\n",
    "except:\n",
    "    os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym\n",
    "\n",
    "import sys\n",
    "sys.path.append('../material')\n",
    "from utils import moving_average\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed_all(123)\n",
    "np.random.seed(123)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyunseok.hwang/anaconda3/envs/RL_scratch/lib/python3.8/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps=2000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRPO or PPO agent 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PO_Agent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions):\n",
    "        super(PO_Agent,self).__init__()\n",
    "        '''\n",
    "        입력변수\n",
    "            state_shape: state 차원 -> [위치, 속도, 각도, 각속도]\n",
    "            output_dim: actor 차원 -> [왼쪽, 오른쪽]\n",
    "                        critic 차원 -> 1\n",
    "            device : cpu, cuda device정보 \n",
    "        N.N 구조\n",
    "            2 - hidden layers, 64 nodes\n",
    "            Activation function -> Relu\n",
    "        '''\n",
    "        self.state_shape = state_shape\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(self.state_shape,128), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(128,self.n_actions)\n",
    "        )\n",
    "            \n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state_t):\n",
    "        '''\n",
    "        입력인자\n",
    "            state_t : 상태([batch,state_shape]), torch.tensor\n",
    "        출력인자\n",
    "            policy : 정책([batch,n_actions]), torch.tensor\n",
    "            value : 가치함수([batch]), torch.tensor\n",
    "        '''\n",
    "        policy = self.policy(self.seq(state_t))\n",
    "        value = self.value(self.seq(state_t)).squeeze(dim=-1)\n",
    "        return policy, value\n",
    "\n",
    "    def sample_actions(self,state_t):\n",
    "        '''\n",
    "        입력인자\n",
    "            state_t : 상태([1,state_shape]), torch.tensor\n",
    "        출력인자\n",
    "            action_t : 행동함수 using torch.multinomial\n",
    "        '''\n",
    "        policy, _ = self.forward(state_t)\n",
    "        policy = torch.squeeze(policy)\n",
    "        softmax_policy = F.softmax(policy,dim=0)\n",
    "        action = torch.multinomial(softmax_policy, num_samples=1).item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.99\n",
    "epsilon = 1e-03\n",
    "delta = 1e-05\n",
    "\n",
    "state = env.reset()\n",
    "num_state = state.shape[0]\n",
    "num_action = env.action_space.n\n",
    "learning_rate = 5e-04\n",
    "max_episode = 1000\n",
    "update_per_episode=10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1 - TRPO vine\n",
    "---\n",
    "Objective function\n",
    "\n",
    "$J = \\mathbb{E}[\\frac{\\pi{(a \\vert s)}}{\\pi_{old}(a \\vert s)}A(s,a)]-KL(\\pi_{old}{( \\cdot \\vert s)}|\\pi( \\cdot \\vert s)) $\n",
    "\n",
    "where,\n",
    "\n",
    "$KL(\\pi_{old}{( \\cdot \\vert s)}|\\pi( \\cdot \\vert s)) \\leq \\delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRPO_agent = PO_Agent(num_state,num_action).to(device)\n",
    "optimizer = optim.Adam(TRPO_agent.parameters(),lr=learning_rate)\n",
    "torch_KL = nn.KLDivLoss(reduction='batchmean')\n",
    "critic_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_reward(rewards,gamma=gamma):\n",
    "    '''\n",
    "    Return 계산함수\n",
    "        target_value = R_t + gamma * R_{t+1} + gamma**2 * R_{t+2} ...\n",
    "    '''\n",
    "    target_value = []\n",
    "    discounted_reward = 0\n",
    "    for reward in reversed(rewards):\n",
    "        discounted_reward = reward + discounted_reward * gamma\n",
    "        target_value.insert(0, discounted_reward)\n",
    "    return target_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1679655290.py, line 82)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [9], line 82\u001b[0;36m\u001b[0m\n\u001b[0;31m    policies, values = #train_agent(states)\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def TRPO_singleloss(transition,train_agent,env,gamma=gamma,delta=delta):\n",
    "    '''\n",
    "    TRPO singleloss함수 계산코드(One step ahead!!)\n",
    "    - 1 step만 앞서 Policy sampling -->> old or current policy\n",
    "    입력인자\n",
    "        batch_sample - 리플레이로부터 받은 샘플(S,A,R,S',done)\n",
    "        train_agent - 훈련에이전트\n",
    "        env - 환경\n",
    "        gamma - 할인율\n",
    "        delta - KL-divergence 범위\n",
    "    출력인자\n",
    "        Total_loss\n",
    "    목적함수 \n",
    "              |Return|                  |Value|                       |Entropy| \n",
    "        (policy/policy_old)*advantage + (value_infer-value_target)**2 + policy*log(policy)\n",
    "    '''\n",
    "    #states,actions,rewards,next_state,done = transition\n",
    "    \n",
    "    #states = torch.Tensor(states).to(device).view(-1,num_state)\n",
    "    #actions = torch.Tensor(actions).to(device).view(-1,num_action)\n",
    "    #rewards = torch.Tensor(rewards[None]).to(device)\n",
    "    #next_state = torch.Tensor([next_state]).to(device).view(-1,num_state)\n",
    "\n",
    "\n",
    "    policies, values = train_agent(states)\n",
    "    _, next_value = train_agent(next_state)\n",
    "    if done:\n",
    "        next_value = 0\n",
    "    \n",
    "    curr_probs = F.softmax(policies,dim=-1)\n",
    "    curr_logprobs = F.log_softmax(policies,dim=-1)\n",
    "\n",
    "    trpo_policy = torch.exp(curr_log_probs - old_logprobs)\n",
    "\n",
    "    #target_values = rewards+gamma*next_value\n",
    "    target_values = discounted_reward(rewards,gamma)\n",
    "    \n",
    "    advantages = target_values - values\n",
    "    entropy = -torch.sum(curr_probs*curr_logprobs,dim=-1)\n",
    "\n",
    "    actor_loss = -torch.mean(trpo_policy*advantages.detach() + epsilon*entropy)\n",
    "    critic_loss = F.mse_loss(target_values.detach(),values)\n",
    "    total_loss = actor_loss + critic_loss\n",
    "    return total_loss, actor_loss, critic_loss\n",
    "\n",
    "def TRPO_vineloss(rollout_transition,train_agent,env,gamma=gamma):\n",
    "    '''\n",
    "    TRPO vineloss함수 계산코드(Roll-out!!!)\n",
    "    - 특정 배치 크기만큼 임의 행동 샘플링 --> old policy  \n",
    "    입력인자\n",
    "        batch_sample - 리플레이로부터 받은 샘플(S,A,R,S',done)\n",
    "        train_agent - 훈련에이전트\n",
    "        env - 환경\n",
    "        gamma - 할인율\n",
    "    출력인자\n",
    "        Total_loss\n",
    "    목적함수 \n",
    "              |Return|                  |Value|                       |Entropy| \n",
    "        (policy/policy_old)*advantage + (value_infer-value_target)**2 + policy*log(policy)\n",
    "    '''\n",
    "    states,actions,rewards,next_state,done = rollout_transition\n",
    "    \n",
    "\n",
    "\n",
    "    #states = torch.Tensor(states).to(device).view(-1,num_state)\n",
    "    #actions = torch.Tensor(actions).to(device).view(-1,num_action)\n",
    "    #rewards = torch.Tensor(rewards[None]).to(device)\n",
    "    #next_state = torch.Tensor([next_state]).to(device).view(-1,num_state)\n",
    "    \n",
    "    policies, values = #train_agent(states)\n",
    "    _, next_value = #train_agent(next_state)\n",
    "    if done:\n",
    "        next_value = 0\n",
    "    \n",
    "    old_policies, old_values = train_agent(states)\n",
    "    old_probs = F.softmax(old_policies,dim=-1)\n",
    "    old_logprobs = F.log_softmax(old_policies,dim=-1)\n",
    "\n",
    "    curr_probs = F.softmax(policies,dim=-1)\n",
    "    curr_logprobs = F.log_softmax(policies,dim=-1)\n",
    "    trpo_policy = torch.exp(curr_log_probs - old_probs)\n",
    "\n",
    "    target_values = discounted_reward(rewards,gamma)\n",
    "    \n",
    "    advantages = target_values - values\n",
    "    entropy = -torch.sum(curr_probs*curr_logprobs,dim=-1)\n",
    "\n",
    "    actor_loss = -torch.mean(trpo_policy*advantages.detach() + epsilon*entropy)\n",
    "    critic_loss = F.mse_loss(target_values.detach(),values)\n",
    "    total_loss = actor_loss + critic_loss\n",
    "    return total_loss, actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRPO_vineupdate(agent,optimizer,env, updates=10, gamma=0.99):\n",
    "    # Vine rollout - env reset된 상태\n",
    "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    for t in range(env._max_episode_steps):\n",
    "        torch_state = torch.Tensor(state).to(device)\n",
    "        torch_state = torch.unsqueeze(torch_state,0)\n",
    "        action = agent.sample_actions(torch_state)\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        #total_reward += reward\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        next_states.append(next_state)\n",
    "        dones.append(done)\n",
    "        \n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Old policy calculation\n",
    "    torch_states = torch.Tensor(states).to(device)\n",
    "    old_policies, old_values = agent(torch_states)\n",
    "    old_probs = F.softmax(old_policies,dim=-1)\n",
    "    old_logprobs = F.log_softmax(old_policies,dim=-1)\n",
    "\n",
    "    # Current policy -> update\n",
    "    for iter in range(updates):\n",
    "        curr_policies, curr_values = agent(torch_states)\n",
    "        curr_probs = F.softmax(curr_policies,dim=-1)\n",
    "        curr_logprobs = F.log_softmax(curr_policies,dim=-1)\n",
    "\n",
    "        Advantage = torch.Tensor(discounted_reward(rewards,gamma)).to(device)-old_values\n",
    "        trpo_goal = torch.exp(curr_logprobs-old_logprobs)\n",
    "        entropy = -torch.sum(curr_probs*curr_logprobs,dim=-1)\n",
    "        KL_div = torch_KL(old_probs, curr_probs)\n",
    "        Critic = critic_loss(Advantage, curr_values)\n",
    "        # Debug\n",
    "        print(f'trpo shape: {trpo_goal.shape}')\n",
    "        print(f'Advantege shape: {Advantage.shape}')\n",
    "        print(f'KL_div shape: {KL_div.shape}')\n",
    "        print(f'Entropy shape: {entropy.shape}')\n",
    "        print(f'Critic shape: {Critic.shape}')\n",
    "        total_loss = torch.mean(-trpo_goal.T*Advantage.detach() + KL_div + entropy) + Critic\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    # log 용도 loss 출력\n",
    "    return agent, Advantage, trpo_goal.mean(), Critic, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trpo shape: torch.Size([41, 2])\n",
      "Advantege shape: torch.Size([41])\n",
      "KL_div shape: torch.Size([])\n",
      "Entropy shape: torch.Size([41])\n",
      "Critic shape: torch.Size([])\n",
      "trpo shape: torch.Size([41, 2])\n",
      "Advantege shape: torch.Size([41])\n",
      "KL_div shape: torch.Size([])\n",
      "Entropy shape: torch.Size([41])\n",
      "Critic shape: torch.Size([])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m cnt \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m TRPO_agent, adv, trpo, critic, total_reward \u001b[39m=\u001b[39m TRPO_vineupdate(TRPO_agent,optimizer,env)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mep: \u001b[39m\u001b[39m{\u001b[39;00mep\u001b[39m}\u001b[39;00m\u001b[39m, total_reward: \u001b[39m\u001b[39m{\u001b[39;00mtotal_reward\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [29], line 48\u001b[0m, in \u001b[0;36mTRPO_vineupdate\u001b[0;34m(agent, optimizer, env, updates, gamma)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCritic shape: \u001b[39m\u001b[39m{\u001b[39;00mCritic\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m     total_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39mtrpo_goal\u001b[39m.\u001b[39mT\u001b[39m*\u001b[39mAdvantage\u001b[39m.\u001b[39mdetach() \u001b[39m+\u001b[39m KL_div \u001b[39m+\u001b[39m entropy) \u001b[39m+\u001b[39m Critic\n\u001b[0;32m---> 48\u001b[0m     total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     49\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     50\u001b[0m \u001b[39m# log 용도 loss 출력\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/RL_scratch/lib/python3.8/site-packages/torch/tensor.py:245\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    238\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    239\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    244\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 245\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/RL_scratch/lib/python3.8/site-packages/torch/autograd/__init__.py:145\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 145\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    146\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    147\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "reward_record, TDloss_record, ACloss_record, CRloss_record = [], [], [], []\n",
    "for ep in range(max_episode):\n",
    "    done = False\n",
    "    \n",
    "    cnt = 0\n",
    "    TRPO_agent, adv, trpo, critic, total_reward = TRPO_vineupdate(TRPO_agent,optimizer,env)\n",
    "    print(f'ep: {ep}, total_reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eda1c160e83cbc6e162d86f3ac820d0e78df5de1466a2ca6fc33fee2ec17e6f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
