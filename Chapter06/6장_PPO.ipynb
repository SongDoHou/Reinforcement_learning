{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym solving - PPO\n",
    "---\n",
    "- PPO(Proximal Policy Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 셀은 필자의 Jupyter notebook 환경 문제로 인해 작성되었습니다\n",
    "import os\n",
    "try:\n",
    "    os.environ[\"DISPLAY\"]\n",
    "except:\n",
    "    os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym\n",
    "\n",
    "import sys\n",
    "sys.path.append('../material')\n",
    "from utils import moving_average, discounted_reward\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed_all(123)\n",
    "np.random.seed(123)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyunseok.hwang/anaconda3/envs/RL_scratch/lib/python3.8/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps=2000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO agent생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO_Actor(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions):\n",
    "        super(PPO_Actor,self).__init__()\n",
    "        '''\n",
    "        입력변수\n",
    "            state_shape: state 차원 -> [위치, 속도, 각도, 각속도]\n",
    "            output_dim: actor 차원 -> [왼쪽, 오른쪽]\n",
    "                        critic 차원 -> 1\n",
    "            device : cpu, cuda device정보 \n",
    "        N.N 구조\n",
    "            2 - hidden layers, 64 nodes\n",
    "            Activation function -> Relu\n",
    "        '''\n",
    "        self.state_shape = state_shape\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(self.state_shape,64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state_t):\n",
    "        '''\n",
    "        입력인자\n",
    "            state_t : 상태([batch,state_shape]), torch.tensor\n",
    "        출력인자\n",
    "            policy : 정책([batch,n_actions]), torch.tensor\n",
    "        '''\n",
    "        policy = self.seq(state_t)\n",
    "        return policy\n",
    "\n",
    "    def sample_actions(self,state_t):\n",
    "        '''\n",
    "        입력인자\n",
    "            state_t : 상태([1,state_shape]), torch.tensor\n",
    "        출력인자\n",
    "            action_t : 행동함수 using torch.multinomial\n",
    "        '''\n",
    "        policy = self.forward(state_t)\n",
    "        policy = torch.squeeze(policy)\n",
    "        softmax_policy = F.softmax(policy,dim=0)\n",
    "        action = torch.multinomial(softmax_policy, num_samples=1).item()\n",
    "        return action\n",
    "\n",
    "class PPO_Critic(nn.Module):\n",
    "    def __init__(self, state_shape):\n",
    "        super(PPO_Critic,self).__init__()\n",
    "        '''\n",
    "        입력 정보 및 출력정보는 Actor와 상당 부분 공유\n",
    "        출력 층만 Value를 계산하도록 변경\n",
    "        '''\n",
    "        self.state_shape = state_shape\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(self.state_shape,64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state_t):\n",
    "        '''\n",
    "        입력인자\n",
    "            state_t : 상태([batch,state_shape]), torch.tensor\n",
    "        출력인자\n",
    "            value : 가치함수([batch]), torch.tensor\n",
    "        '''\n",
    "        value = self.seq(state_t).squeeze(dim=-1)\n",
    "        return value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.99\n",
    "epsilon = 1e-03\n",
    "\n",
    "state = env.reset()\n",
    "num_state = state.shape[0]\n",
    "num_action = env.action_space.n\n",
    "learning_rate = 5e-04\n",
    "max_episode = 1000\n",
    "update_per_episode=10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clipped Surrogate Objective\n",
    "---\n",
    "Objective function\n",
    "\n",
    "$$\n",
    "\n",
    "J(\\theta) = \\mathbb{E}_t[min(r_t(\\theta)\\hat{A}_t,clip(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t)]\n",
    "\n",
    "$$\n",
    "\n",
    "where,\n",
    "$r_t(\\theta) = \\frac{\\pi{(a_t \\vert s_t)}}{\\pi_{old}(a_t \\vert s_t)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = PPO_Actor(num_state,num_action).to(device)\n",
    "critic = PPO_Critic(num_state).to(device)\n",
    "actor_optimizer = optim.Adam(actor.parameters(),lr=learning_rate)\n",
    "critic_optimizer = optim.Adam(critic.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var: tensor([0.5000, 0.5000])\n",
      "mat: tensor([[0.5000, 0.0000],\n",
      "        [0.0000, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "var = torch.full(size=(2,),fill_value=0.5)\n",
    "mat = torch.diag(var)\n",
    "print(f'var: {var}')\n",
    "print(f'mat: {mat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPO_rollout(env,actor):\n",
    "    # rollout - env reset된 상태\n",
    "    states,  rewards, policies = [], [], []\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    for t in range(env._max_episode_steps):\n",
    "        torch_state = torch.Tensor(state).to(device)\n",
    "        torch_state = torch.unsqueeze(torch_state,0)\n",
    "        policy = actor(torch_state).detach().cpu().numpy().ravel()\n",
    "        action = actor.sample_actions(torch_state)\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        \n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        policies.append(policy)\n",
    "        \n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "    learning_history = (states,rewards,policies)\n",
    "    return learning_history, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPO_training(history,actor,critic,updates=10, clips=0.2):\n",
    "    states, rewards, policies = history\n",
    "    torch_states = torch.Tensor(states).to(device)\n",
    "    torch_Advantage = torch.Tensor(discounted_reward(rewards,gamma)).to(device)\n",
    "    old_policies = torch.Tensor(policies).to(device)\n",
    "    old_logpolicy = F.log_softmax(old_policies,dim=-1)\n",
    "\n",
    "    value = critic(torch_states)\n",
    "    Advantage = torch_Advantage - value.detach()\n",
    "    # Advantage - 정규화 학습의 안정성\n",
    "    Advantage = (Advantage - Advantage.mean())/(Advantage + 1e-10)\n",
    "    #Advantage = Advantage.unsqueeze(dim=0)\n",
    "\n",
    "    for i in range(updates):\n",
    "        value = critic(torch_states)\n",
    "        new_policies = actor(torch_states)\n",
    "        new_logpolicy = F.log_softmax(new_policies,dim=-1)\n",
    "\n",
    "        rt = torch.exp(new_logpolicy - old_logpolicy)\n",
    "        print(f'rt shape: {rt.shape}')\n",
    "        print(f'Advantage shape: {Advantage.shape}')\n",
    "\n",
    "        candid_1 = rt*Advantage\n",
    "        candid_2 = torch.clip(rt, 1-clips, 1+clips)*Advantage\n",
    "        \n",
    "        # 목적함수 설정\n",
    "        actor_loss = -torch.mean(torch.min(candid_1, candid_2))\n",
    "        critic_loss = torch.mean((Advantage - value)**2)\n",
    "\n",
    "        # 최적화 시행\n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        #actor_loss.backward(retain_graph=True)\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        critic_optimizer.step()\n",
    "    return actor, critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt shape: torch.Size([57, 2])\n",
      "Advantage shape: torch.Size([1, 57])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (57) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m ep \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_episode):\n\u001b[1;32m      3\u001b[0m     history, total_reward \u001b[39m=\u001b[39m PPO_rollout(env,actor)\n\u001b[0;32m----> 4\u001b[0m     actor,critic \u001b[39m=\u001b[39m PPO_training(history, actor, critic)\n\u001b[1;32m      5\u001b[0m     reward_record\u001b[39m.\u001b[39mappend(total_reward)\n\u001b[1;32m      7\u001b[0m     \u001b[39mif\u001b[39;00m ep \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn [18], line 23\u001b[0m, in \u001b[0;36mPPO_training\u001b[0;34m(history, actor, critic, updates, clips)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrt shape: \u001b[39m\u001b[39m{\u001b[39;00mrt\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAdvantage shape: \u001b[39m\u001b[39m{\u001b[39;00mAdvantage\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m candid_1 \u001b[39m=\u001b[39m rt\u001b[39m*\u001b[39;49mAdvantage\n\u001b[1;32m     24\u001b[0m candid_2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclip(rt, \u001b[39m1\u001b[39m\u001b[39m-\u001b[39mclips, \u001b[39m1\u001b[39m\u001b[39m+\u001b[39mclips)\u001b[39m*\u001b[39mAdvantage\n\u001b[1;32m     26\u001b[0m \u001b[39m# 목적함수 설정\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (57) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "reward_record = []\n",
    "for ep in range(max_episode):\n",
    "    history, total_reward = PPO_rollout(env,actor)\n",
    "    actor,critic = PPO_training(history, actor, critic)\n",
    "    reward_record.append(total_reward)\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        clear_output(True)\n",
    "        print(f'{ep}번째 에피소드 결과')\n",
    "        print(f'최근 10 에피소드 보상평균 = {np.mean(reward_record[-10:])}')\n",
    "\n",
    "        plt.figure(figsize=[16, 18])\n",
    "        \n",
    "        plt.subplot(1,1,1)\n",
    "        plt.title(\"Total Reward\")\n",
    "        plt.plot(reward_record)\n",
    "        plt.plot(moving_average(reward_record))\n",
    "        plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eda1c160e83cbc6e162d86f3ac820d0e78df5de1466a2ca6fc33fee2ec17e6f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
