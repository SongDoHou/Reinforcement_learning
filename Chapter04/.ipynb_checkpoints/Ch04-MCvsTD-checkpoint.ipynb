{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833861e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.24.1.tar.gz (696 kB)\n",
      "\u001b[K     |████████████████████████████████| 696 kB 12.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /home/hshwang/anaconda3/envs/torch_1.8/lib/python3.8/site-packages (from gym) (1.19.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/hshwang/anaconda3/envs/torch_1.8/lib/python3.8/site-packages (from gym) (4.10.0)\n",
      "Collecting cloudpickle>=1.2.0\n",
      "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.7-py3-none-any.whl (2.7 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/hshwang/anaconda3/envs/torch_1.8/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym) (3.7.0)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.24.1-py3-none-any.whl size=793137 sha256=1a6e0d518bd75f98673b92275f2f3f9ba94cfa1b1807d6b6a6a4aa0b5b439e2c\n",
      "  Stored in directory: /home/hshwang/.cache/pip/wheels/5a/e9/0b/5536e77ed2edbbf067ecff287ec039633d40daee4d8dac7716\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-2.1.0 gym-0.24.1 gym-notices-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ceaa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3fee0a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "52ecbca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에피소드 종료까지 최대 상한 스텝: 1000\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env._max_episode_steps=1000\n",
    "print(f'에피소드 종료까지 최대 상한 스텝: {env._max_episode_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0d9ee6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CartPoleEnv in module gym.envs.classic_control.cartpole object:\n",
      "\n",
      "class CartPoleEnv(gym.core.Env)\n",
      " |  CartPoleEnv(*args, **kwds)\n",
      " |  \n",
      " |  ### Description\n",
      " |  \n",
      " |  This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
      " |  [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
      " |  A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
      " |  The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
      " |   in the left and right direction on the cart.\n",
      " |  \n",
      " |  ### Action Space\n",
      " |  \n",
      " |  The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
      " |   of the fixed force the cart is pushed with.\n",
      " |  \n",
      " |  | Num | Action                 |\n",
      " |  |-----|------------------------|\n",
      " |  | 0   | Push cart to the left  |\n",
      " |  | 1   | Push cart to the right |\n",
      " |  \n",
      " |  **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
      " |   the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
      " |  \n",
      " |  ### Observation Space\n",
      " |  \n",
      " |  The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
      " |  \n",
      " |  | Num | Observation           | Min                 | Max               |\n",
      " |  |-----|-----------------------|---------------------|-------------------|\n",
      " |  | 0   | Cart Position         | -4.8                | 4.8               |\n",
      " |  | 1   | Cart Velocity         | -Inf                | Inf               |\n",
      " |  | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
      " |  | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
      " |  \n",
      " |  **Note:** While the ranges above denote the possible values for observation space of each element,\n",
      " |      it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
      " |  -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
      " |     if the cart leaves the `(-2.4, 2.4)` range.\n",
      " |  -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
      " |     if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
      " |  \n",
      " |  ### Rewards\n",
      " |  \n",
      " |  Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken,\n",
      " |  including the termination step, is allotted. The threshold for rewards is 475 for v1.\n",
      " |  \n",
      " |  ### Starting State\n",
      " |  \n",
      " |  All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
      " |  \n",
      " |  ### Episode Termination\n",
      " |  \n",
      " |  The episode terminates if any one of the following occurs:\n",
      " |  1. Pole Angle is greater than ±12°\n",
      " |  2. Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
      " |  3. Episode length is greater than 500 (200 for v0)\n",
      " |  \n",
      " |  ### Arguments\n",
      " |  \n",
      " |  ```\n",
      " |  gym.make('CartPole-v1')\n",
      " |  ```\n",
      " |  \n",
      " |  No additional arguments are currently supported.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CartPoleEnv\n",
      " |      gym.core.Env\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  close(self)\n",
      " |      Override close in your subclass to perform any necessary cleanup.\n",
      " |      \n",
      " |      Environments will automatically :meth:`close()` themselves when\n",
      " |      garbage collected or when the program exits.\n",
      " |  \n",
      " |  render(self, mode='human')\n",
      " |      Renders the environment.\n",
      " |      \n",
      " |      A set of supported modes varies per environment. (And some\n",
      " |      third-party environments may not support rendering at all.)\n",
      " |      By convention, if mode is:\n",
      " |      \n",
      " |      - human: render to the current display or terminal and\n",
      " |        return nothing. Usually for human consumption.\n",
      " |      - rgb_array: Return a numpy.ndarray with shape (x, y, 3),\n",
      " |        representing RGB values for an x-by-y pixel image, suitable\n",
      " |        for turning into a video.\n",
      " |      - ansi: Return a string (str) or StringIO.StringIO containing a\n",
      " |        terminal-style text representation. The text can include newlines\n",
      " |        and ANSI escape sequences (e.g. for colors).\n",
      " |      \n",
      " |      Note:\n",
      " |          Make sure that your class's metadata 'render_modes' key includes\n",
      " |          the list of supported modes. It's recommended to call super()\n",
      " |          in implementations to use the functionality of this method.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> import numpy as np\n",
      " |          >>> class MyEnv(Env):\n",
      " |          ...    metadata = {'render_modes': ['human', 'rgb_array']}\n",
      " |          ...\n",
      " |          ...    def render(self, mode='human'):\n",
      " |          ...        if mode == 'rgb_array':\n",
      " |          ...            return np.array(...) # return RGB frame suitable for video\n",
      " |          ...        elif mode == 'human':\n",
      " |          ...            ... # pop up a window and render\n",
      " |          ...        else:\n",
      " |          ...            super().render(mode=mode) # just raise an exception\n",
      " |      \n",
      " |      Args:\n",
      " |          mode: the mode to render with, valid modes are `env.metadata[\"render_modes\"]`\n",
      " |  \n",
      " |  reset(self, *, seed: Union[int, NoneType] = None, return_info: bool = False, options: Union[dict, NoneType] = None)\n",
      " |      Resets the environment to an initial state and returns the initial observation.\n",
      " |      \n",
      " |      This method can reset the environment's random number generator(s) if ``seed`` is an integer or\n",
      " |      if the environment has not yet initialized a random number generator.\n",
      " |      If the environment already has a random number generator and :meth:`reset` is called with ``seed=None``,\n",
      " |      the RNG should not be reset. Moreover, :meth:`reset` should (in the typical use case) be called with an\n",
      " |      integer seed right after initialization and then never again.\n",
      " |      \n",
      " |      Args:\n",
      " |          seed (optional int): The seed that is used to initialize the environment's PRNG.\n",
      " |              If the environment does not already have a PRNG and ``seed=None`` (the default option) is passed,\n",
      " |              a seed will be chosen from some source of entropy (e.g. timestamp or /dev/urandom).\n",
      " |              However, if the environment already has a PRNG and ``seed=None`` is passed, the PRNG will *not* be reset.\n",
      " |              If you pass an integer, the PRNG will be reset even if it already exists.\n",
      " |              Usually, you want to pass an integer *right after the environment has been initialized and then never again*.\n",
      " |              Please refer to the minimal example above to see this paradigm in action.\n",
      " |          return_info (bool): If true, return additional information along with initial observation.\n",
      " |              This info should be analogous to the info returned in :meth:`step`\n",
      " |          options (optional dict): Additional information to specify how the environment is reset (optional,\n",
      " |              depending on the specific environment)\n",
      " |      \n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): Observation of the initial state. This will be an element of :attr:`observation_space`\n",
      " |              (typically a numpy array) and is analogous to the observation returned by :meth:`step`.\n",
      " |          info (optional dictionary): This will *only* be returned if ``return_info=True`` is passed.\n",
      " |              It contains auxiliary information complementing ``observation``. This dictionary should be analogous to\n",
      " |              the ``info`` returned by :meth:`step`.\n",
      " |  \n",
      " |  step(self, action)\n",
      " |      Run one timestep of the environment's dynamics.\n",
      " |      \n",
      " |      When end of episode is reached, you are responsible for calling :meth:`reset` to reset this environment's state.\n",
      " |      Accepts an action and returns a tuple `(observation, reward, done, info)`.\n",
      " |      \n",
      " |      Args:\n",
      " |          action (ActType): an action provided by the agent\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): this will be an element of the environment's :attr:`observation_space`.\n",
      " |              This may, for instance, be a numpy array containing the positions and velocities of certain objects.\n",
      " |          reward (float): The amount of reward returned as a result of taking the action.\n",
      " |          done (bool): A boolean value for if the episode has ended, in which case further :meth:`step` calls will return undefined results.\n",
      " |              A done signal may be emitted for different reasons: Maybe the task underlying the environment was solved successfully,\n",
      " |              a certain timelimit was exceeded, or the physics simulation has entered an invalid state.\n",
      " |          info (dictionary): A dictionary that may contain additional information regarding the reason for a ``done`` signal.\n",
      " |              `info` contains auxiliary diagnostic information (helpful for debugging, learning, and logging).\n",
      " |              This might, for instance, contain: metrics that describe the agent's performance state, variables that are\n",
      " |              hidden from observations, information that distinguishes truncation and termination or individual reward terms\n",
      " |              that are combined to produce the total reward\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __orig_bases__ = (gym.core.Env[numpy.ndarray, typing.Union[numpy.ndarr...\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  metadata = {'render_fps': 50, 'render_modes': ['human', 'rgb_array']}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.core.Env:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Returns a string of the environment with the spec id if specified.\n",
      " |  \n",
      " |  seed(self, seed=None)\n",
      " |      :deprecated: function that sets the seed for the environment's random number generator(s).\n",
      " |      \n",
      " |      Use `env.reset(seed=seed)` as the new API for setting the seed of the environment.\n",
      " |      \n",
      " |      Note:\n",
      " |          Some environments use multiple pseudorandom number generators.\n",
      " |          We want to capture all such seeds used in order to ensure that\n",
      " |          there aren't accidental correlations between multiple generators.\n",
      " |      \n",
      " |      Args:\n",
      " |          seed(Optional int): The seed value for the random number geneartor\n",
      " |      \n",
      " |      Returns:\n",
      " |          seeds (List[int]): Returns the list of seeds used in this environment's random\n",
      " |            number generators. The first value in the list should be the\n",
      " |            \"main\" seed, or the value which a reproducer should pass to\n",
      " |            'seed'. Often, the main seed equals the provided 'seed', but\n",
      " |            this won't be true `if seed=None`, for example.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from gym.core.Env:\n",
      " |  \n",
      " |  unwrapped\n",
      " |      Returns the base non-wrapped environment.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Env: The base non-wrapped gym.Env instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gym.core.Env:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  np_random\n",
      " |      Returns the environment's internal :attr:`_np_random` that if not set will initialise with a random seed.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from gym.core.Env:\n",
      " |  \n",
      " |  __annotations__ = {'_np_random': typing.Union[gym.utils.seeding.Random...\n",
      " |  \n",
      " |  reward_range = (-inf, inf)\n",
      " |  \n",
      " |  spec = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwds)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(env.unwrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d37ce059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관찰의 차원: 4\n",
      "취할 수 있는 행동: 2\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "n_state = state.shape[0]\n",
    "n_action = env.action_space.n\n",
    "print(f'관찰의 차원: {n_state}')\n",
    "print(f'취할 수 있는 행동: {n_action}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c3893ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_model(nn.Module):\n",
    "    def __init__(self,input_dim=n_state,output_dim=n_action):\n",
    "        super(NN_model,self).__init__()\n",
    "        '''\n",
    "        입력변수\n",
    "            input_dim: state의 차원 -> cartpole [위치, 속도, 각도, 각속도]\n",
    "            output_dim: action의 차원 -> cartpole [왼쪽, 오른쪽]\n",
    "                        critic의 차원 -> 1\n",
    "        N.N 구조\n",
    "            4 layer구조 (2 hidden layer).\n",
    "            hidden node개수는 64개로 통일.\n",
    "            activation function은 Relu 설정\n",
    "        '''\n",
    "        self.lin1 = nn.Linear(input_dim,64)\n",
    "        self.lin2 = nn.Linear(64,64)\n",
    "        self.policy_out = nn.Linear(64,output_dim)\n",
    "        self.value_out = nn.Linear(64,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        policy = self.policy_out(x)\n",
    "        value = self.value_out(x).squeeze(dim=-1)\n",
    "        return policy, value\n",
    "    \n",
    "    def get_action(self,x):\n",
    "        if x.shape == 1:\n",
    "            x = torch.FloatTensor(x[None]).to(device)\n",
    "        else:\n",
    "            x = torch.FloatTensor(x).to(device)\n",
    "        policy,_ = self.forward(x)\n",
    "        policy = F.softmax(policy,dim=-1).detach().cpu().numpy().ravel()\n",
    "        actions = np.random.choice(n_action, p=policy) \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "21bd769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def A2C_loss(state,action,rewards,next_state,done,gamma=0.99,agent=agent):\n",
    "    '''\n",
    "    목적: A2C loss계산후 agent 학습\n",
    "        목적함수: -log(policy)*value + (value_infer-value_target)**2 + policy*log(policy)\n",
    "            \"-log(policy)*value\": Actor-loss(exploitation)\n",
    "            \"policy*log(policy)\": Actor-entropy(exploration)\n",
    "            \"(value_infer-value_target)**2\": Critic-loss\n",
    "    입력인자\n",
    "        state: 상태 [1,4]\n",
    "        action: 액션 [1,2]\n",
    "        rewards: 보상 - TD step에 따라 step의 수가 정해져 있음 [TD_step]\n",
    "        next_state: 다음상태 [1,4]\n",
    "        done: 종료여부 \n",
    "        gamma: discount factor(할인율)\n",
    "        agent: agent\n",
    "    출력인자\n",
    "        total_loss: 모니터링을 위한 total_loss\n",
    "    '''\n",
    "    def Calculate_target(rewards,gamma):\n",
    "        '''\n",
    "        목표\n",
    "            G_t = R_{t+1}+gamma*R_{t+2}+gamma**2+R_{t+3}+...\n",
    "        '''\n",
    "        returns = rewards[-1] \n",
    "        for reward in reversed(rewards[:-1]):\n",
    "            returns = gamma*returns+reward\n",
    "        return returns.view(-1,)\n",
    "    \n",
    "    states = torch.FloatTensor(state).to(device)\n",
    "    actions = torch.LongTensor(action).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_state).to(device)\n",
    "    dones = torch.tensor(done,dtype=torch.uint8).to(device)\n",
    "    \n",
    "    policy,value = agent(states)\n",
    "    next_policy,next_value = agent(next_states)\n",
    "    \n",
    "    probs = F.softmax(policy,dim=-1)\n",
    "    logprobs = F.log_softmax(policy,dim=-1)\n",
    "    next_value[done] = 0.\n",
    "    \n",
    "    target_value = Calculate_target(rewards,gamma)\n",
    "    advantage = target_value-value\n",
    "    \n",
    "    logp_actions = logprobs[np.arange(states.shape[0]),actions]\n",
    "    \n",
    "    entropy = -probs*logprobs\n",
    "    actor_loss = -(logp_actions*advantage.detach()).mean()-0.001*entropy.mean()\n",
    "    critic_loss = F.mse_loss(target_value.detach(),value)\n",
    "    total_loss = actor_loss + critic_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25320f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_or_train_agent(env,agent,optimizer,train=True,TD_step=0):\n",
    "    '''\n",
    "    목표: agent를 환경에 맞게 train할것인지 단순 play할것인지 구성\n",
    "    입력인자\n",
    "        env: environment(CartPole)\n",
    "        agent: agnet\n",
    "        train: 학습여부(True: 학습, False: play)\n",
    "        TD_step: TemporalDifference(0) -> MonteCarlo(-1)\n",
    "    출력인자\n",
    "        total_reward: \n",
    "        loss: 모니터링을 위한 TD error의 추이\n",
    "    '''\n",
    "    \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:    \n",
    "        action = agent.get_action(state)#[0]\n",
    "\n",
    "        states, actions, rewards, new_states, dones = [],[],[],[],[]\n",
    "        # action\n",
    "        if TD_step != -1:\n",
    "            for it in range(TD_step+1):\n",
    "                new_state,reward,done,_ = env.step(action)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                new_states.append(new_state)\n",
    "                dones.append(done)\n",
    "                state = new_state.copy()\n",
    "                if done:\n",
    "                    break\n",
    "                total_reward += reward\n",
    "            states.append(state)\n",
    "        else:\n",
    "            while not done:\n",
    "                new_state,reward,done,_ = env.step(action)  \n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                new_states.append(new_state)\n",
    "                dones.append(done)\n",
    "                \n",
    "                total_reward += reward\n",
    "                state = new_state.copy()\n",
    "             states.append(state)    \n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss = A2C_loss(states,actions,rewards,new_states,dones,agent)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    if train:\n",
    "        return total_reward, loss.item()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2421afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "\n",
    "agent = NN_model().to(device)\n",
    "optimizer = optim.Adam(agent.parameters(),lr=1e-04)\n",
    "\n",
    "episode_rewards = []\n",
    "max_episodes = 100\n",
    "mini_sessions = 50\n",
    "\n",
    "for episode in trange(max_episodes):\n",
    "    mini_reward = []\n",
    "    for mini_session in range(mini_sessions):\n",
    "        episode_reward, total_loss = play_or_train_agent(env,agent=agent,optimizer=optimizer,train=True)\n",
    "        mini_reward.append(episode_reward)\n",
    "    \n",
    "    episode_rewards.append(np.mean(mini_reward))\n",
    "    A2C_losses.append(np.mean(mini_td))\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Episode\",episode)\n",
    "    \n",
    "    plt.figure(figsize=[12, 10])\n",
    "    plt.title(\"Total reward per each episode\")\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    if np.mean(mini_reward) >= 500:\n",
    "        print(f\"Agent finds solution! Final score : {np.mean(mini_reward)}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df98b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a647c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.8",
   "language": "python",
   "name": "torch_1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
